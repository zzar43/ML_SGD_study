{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6GO7bsZLRPn"
      },
      "source": [
        "# ML-SGD MAIN worker 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBKoEz-cLQwX",
        "outputId": "0fbe4e11-63f8-4ce7-a923-4f4ea52bb3a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Wed Jan 19 17:45:49 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    24W / 300W |      2MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/MLSGD/ex_data\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "# from scipy.stats import multivariate_normal\n",
        "from copy import deepcopy\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# check GPU info\n",
        "!/opt/bin/nvidia-smi\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAVQ6Hy-LUNF"
      },
      "outputs": [],
      "source": [
        "import random, os\n",
        "def seed_torch(seed=33):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.enabled = False\n",
        "seed_torch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emnxh8m6LevW",
        "outputId": "ee57958b-6cd8-4350-8147-95af030640a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# define transforms\n",
        "resize_32 = transforms.Compose([transforms.Resize((32, 32)),\n",
        "                                 transforms.ToTensor()])\n",
        "\n",
        "# import data\n",
        "training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=resize_32\n",
        ")\n",
        "\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=resize_32\n",
        ")\n",
        "\n",
        "Nr = 10\n",
        "batch_size = 16\n",
        "batch_size = batch_size * Nr\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, worker_init_fn=torch.manual_seed(33))\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "fH2i8S4ZLtgP",
        "outputId": "4b928204-75ee-40db-88e1-cc2d237a3288"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '1')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "img, label = training_data[4]\n",
        "img1 = np.ndarray([32,32,3])\n",
        "for i in range(3):\n",
        "    img1[:,:,i] = img[i,:,:]\n",
        "\n",
        "plt.imshow(img1)\n",
        "plt.title(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBIy3W-aLv0O"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "\n",
        "    def __init__(self, n_classes):\n",
        "        super(LeNet5, self).__init__()\n",
        "        \n",
        "        self.feature_extractor = nn.Sequential(            \n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=120, out_features=84),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=84, out_features=n_classes),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.classifier(x)\n",
        "        # probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "        return logits\n",
        "\n",
        "# figure out what is probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsmKFl_QY3-p",
        "outputId": "fe58f6a2-a551-43ff-96c9-aeaaf0dd747f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n",
            "10\n",
            "0\n",
            "tensor([0, 7, 1, 7, 9, 1, 9, 8, 2, 5, 9, 4, 0, 2, 5, 7])\n",
            "tensor([5, 1, 7, 7, 7, 0, 6, 1, 4, 3, 0, 2, 9, 8, 5, 5])\n",
            "tensor([8, 1, 2, 0, 1, 6, 4, 0, 1, 1, 5, 7, 2, 7, 1, 8])\n",
            "tensor([7, 0, 0, 5, 6, 7, 8, 7, 8, 5, 2, 4, 5, 6, 9, 2])\n",
            "tensor([8, 8, 7, 8, 4, 5, 6, 3, 1, 4, 0, 4, 8, 5, 0, 3])\n",
            "tensor([6, 3, 7, 9, 9, 9, 1, 9, 5, 8, 7, 8, 9, 9, 8, 7])\n",
            "tensor([4, 5, 6, 2, 5, 5, 2, 0, 7, 8, 5, 0, 9, 4, 1, 6])\n",
            "tensor([6, 8, 0, 4, 2, 6, 1, 9, 5, 6, 8, 4, 0, 3, 2, 9])\n",
            "tensor([9, 6, 8, 7, 7, 2, 8, 9, 5, 8, 9, 2, 0, 4, 0, 9])\n",
            "tensor([9, 3, 9, 4, 7, 4, 4, 0, 8, 0, 9, 0, 0, 9, 5, 9])\n",
            "tensor([0, 7, 1, 7, 9, 1, 9, 8, 2, 5, 9, 4, 0, 2, 5, 7, 5, 1, 7, 7, 7, 0, 6, 1,\n",
            "        4, 3, 0, 2, 9, 8, 5, 5, 8, 1, 2, 0, 1, 6, 4, 0, 1, 1, 5, 7, 2, 7, 1, 8,\n",
            "        7, 0, 0, 5, 6, 7, 8, 7, 8, 5, 2, 4, 5, 6, 9, 2, 8, 8, 7, 8, 4, 5, 6, 3,\n",
            "        1, 4, 0, 4, 8, 5, 0, 3, 6, 3, 7, 9, 9, 9, 1, 9, 5, 8, 7, 8, 9, 9, 8, 7,\n",
            "        4, 5, 6, 2, 5, 5, 2, 0, 7, 8, 5, 0, 9, 4, 1, 6, 6, 8, 0, 4, 2, 6, 1, 9,\n",
            "        5, 6, 8, 4, 0, 3, 2, 9, 9, 6, 8, 7, 7, 2, 8, 9, 5, 8, 9, 2, 0, 4, 0, 9,\n",
            "        9, 3, 9, 4, 7, 4, 4, 0, 8, 0, 9, 0, 0, 9, 5, 9])\n"
          ]
        }
      ],
      "source": [
        "N = 0\n",
        "batch_size_mlsgd = batch_size // Nr\n",
        "print(batch_size_mlsgd)\n",
        "print(batch_size // batch_size_mlsgd)\n",
        "\n",
        "\n",
        "for batch, (X, y) in enumerate(train_dataloader):\n",
        "    print(batch)\n",
        "\n",
        "    for i in range(batch_size // batch_size_mlsgd):\n",
        "        XX = X[i*batch_size_mlsgd:(i+1)*batch_size_mlsgd]\n",
        "        yy = y[i*batch_size_mlsgd:(i+1)*batch_size_mlsgd]\n",
        "        print(yy)\n",
        "\n",
        "    # print(X)\n",
        "    print(y)\n",
        "    N += 1\n",
        "    if N == 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W677KRItaHRO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVOF25fYaHTr"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SguphOgaHWB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_vG7i9MaHYU"
      },
      "outputs": [],
      "source": [
        "class GaussianPertubation(object):\n",
        "    def __init__(self,std = 1.,device=device):\n",
        "        self.std = std\n",
        "        self.device = device\n",
        "    def __call__(self, module):\n",
        "        # filter the variables to get the ones you want\n",
        "        if hasattr(module, 'weight'):\n",
        "            w = module.weight.data\n",
        "            w += torch.normal(0,self.std,w.shape,device=device)\n",
        "            module.weight.data = w\n",
        "        if hasattr(module, 'bias'):\n",
        "            b = module.bias.data\n",
        "            b += torch.normal(0,self.std,b.shape,device=device)\n",
        "            module.bias.data = b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARnYB9mfixDF",
        "outputId": "ad7f2edb-eb82-4845-827f-563c9dbb5383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-6.9087e-02,  4.5489e-02,  3.1575e-02, -8.1948e-02,  7.4200e-02],\n",
            "         [-6.0863e-02, -2.9971e-02,  9.6263e-02, -1.0777e-01,  5.0404e-02],\n",
            "         [-1.0796e-01, -6.1882e-02, -7.0343e-02,  8.1653e-02,  1.5372e-02],\n",
            "         [-5.7408e-02, -1.0465e-01,  4.5553e-02,  4.8515e-02,  4.7133e-03],\n",
            "         [-3.0775e-02,  2.5680e-02,  1.0630e-01, -1.5986e-02,  9.0205e-02]],\n",
            "\n",
            "        [[ 9.6120e-02,  4.4042e-02,  7.4594e-02, -1.3668e-02, -8.7156e-02],\n",
            "         [-4.5614e-03,  7.3643e-06,  3.3617e-02, -1.0098e-01,  7.0700e-03],\n",
            "         [ 4.4584e-02,  1.0270e-02, -8.2620e-02, -1.6470e-02, -8.4654e-02],\n",
            "         [-1.5781e-02, -1.3366e-02, -7.6450e-03,  1.0797e-01,  2.6048e-04],\n",
            "         [ 1.8904e-02,  6.7374e-02,  4.6198e-02, -9.4938e-02,  6.4232e-02]],\n",
            "\n",
            "        [[-5.3348e-02,  1.1316e-01, -4.3093e-03,  2.2980e-02, -9.5287e-02],\n",
            "         [-1.3857e-02, -2.4441e-02,  8.1819e-03, -4.1158e-02,  7.4328e-02],\n",
            "         [-1.0321e-01,  1.0983e-01,  8.2687e-02, -6.9116e-02,  2.7584e-02],\n",
            "         [-1.6153e-02, -6.1948e-02, -3.1243e-02, -4.2936e-02, -1.7300e-02],\n",
            "         [-1.3075e-02,  5.8777e-02,  1.8679e-02,  9.2564e-02, -7.0258e-02]]],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "tensor([[[-0.0691,  0.0455,  0.0315, -0.0819,  0.0740],\n",
            "         [-0.0608, -0.0301,  0.0962, -0.1076,  0.0505],\n",
            "         [-0.1081, -0.0618, -0.0705,  0.0816,  0.0152],\n",
            "         [-0.0574, -0.1047,  0.0455,  0.0486,  0.0048],\n",
            "         [-0.0307,  0.0258,  0.1063, -0.0159,  0.0901]],\n",
            "\n",
            "        [[ 0.0960,  0.0443,  0.0746, -0.0137, -0.0871],\n",
            "         [-0.0047, -0.0001,  0.0336, -0.1010,  0.0071],\n",
            "         [ 0.0445,  0.0104, -0.0825, -0.0167, -0.0846],\n",
            "         [-0.0159, -0.0134, -0.0077,  0.1081,  0.0003],\n",
            "         [ 0.0186,  0.0674,  0.0461, -0.0950,  0.0642]],\n",
            "\n",
            "        [[-0.0534,  0.1130, -0.0043,  0.0229, -0.0951],\n",
            "         [-0.0136, -0.0245,  0.0082, -0.0412,  0.0743],\n",
            "         [-0.1032,  0.1098,  0.0827, -0.0692,  0.0275],\n",
            "         [-0.0162, -0.0619, -0.0313, -0.0429, -0.0173],\n",
            "         [-0.0132,  0.0587,  0.0188,  0.0925, -0.0703]]], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([[[-0.0691,  0.0455,  0.0315, -0.0820,  0.0739],\n",
            "         [-0.0609, -0.0301,  0.0961, -0.1075,  0.0504],\n",
            "         [-0.1081, -0.0618, -0.0704,  0.0816,  0.0151],\n",
            "         [-0.0573, -0.1046,  0.0453,  0.0485,  0.0048],\n",
            "         [-0.0308,  0.0258,  0.1062, -0.0160,  0.0901]],\n",
            "\n",
            "        [[ 0.0962,  0.0443,  0.0747, -0.0137, -0.0871],\n",
            "         [-0.0048, -0.0001,  0.0336, -0.1008,  0.0071],\n",
            "         [ 0.0444,  0.0103, -0.0825, -0.0166, -0.0847],\n",
            "         [-0.0159, -0.0135, -0.0079,  0.1083,  0.0003],\n",
            "         [ 0.0185,  0.0674,  0.0462, -0.0952,  0.0642]],\n",
            "\n",
            "        [[-0.0533,  0.1129, -0.0045,  0.0228, -0.0952],\n",
            "         [-0.0134, -0.0247,  0.0083, -0.0413,  0.0743],\n",
            "         [-0.1031,  0.1098,  0.0827, -0.0693,  0.0275],\n",
            "         [-0.0162, -0.0619, -0.0314, -0.0429, -0.0174],\n",
            "         [-0.0130,  0.0586,  0.0187,  0.0924, -0.0702]]], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "109.95116277760006\n"
          ]
        }
      ],
      "source": [
        "model = LeNet5(10)\n",
        "model.cuda()\n",
        "nu = 1e2 * 1.6**10\n",
        "GP = GaussianPertubation(std=1/nu)\n",
        "\n",
        "for m in model.parameters():\n",
        "    print(m[0])\n",
        "    break\n",
        "\n",
        "model.apply(GP)\n",
        "for m in model.parameters():\n",
        "    print(m[0])\n",
        "    break\n",
        "\n",
        "\n",
        "model.apply(GP)\n",
        "for m in model.parameters():\n",
        "    print(m[0])\n",
        "    break\n",
        "\n",
        "print(1.6 ** 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miJYv9xpLywd"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        X = X.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 1000 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train_loop_MLSGD_fixed(dataloader, model, loss_fn, lr, Nr, nu):\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    GP = GaussianPertubation(std=1/nu)\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        X = X.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        model_fix = deepcopy(model)\n",
        "\n",
        "        for ind in range(Nr):\n",
        "            model_rand = deepcopy(model_fix)\n",
        "            model_rand.apply(GP)\n",
        "\n",
        "            pred = model_rand(X)\n",
        "            loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "\n",
        "            count = 0\n",
        "            for p in model.parameters():\n",
        "                p.data -= lr * list(model_rand.parameters())[count].grad.data\n",
        "                # p.data -= lr * list(model_rand.parameters())[count].grad.data / Nr\n",
        "                count += 1\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train_loop_MLSGD(dataloader, model, loss_fn, lr, Nr, batch_size, nu):\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    GP = GaussianPertubation(std=1/nu)\n",
        "\n",
        "    batch_size_mlsgd = batch_size // Nr\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        X = X.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        # model: the main updated model, all updates on model\n",
        "        # model_fix: save the model at the beginning of each iteration\n",
        "        # model_rand: evaluate the loss and gradient for the Gaussian perturbated model\n",
        "\n",
        "        model_fix = deepcopy(model)\n",
        "\n",
        "        for i in range(batch_size // batch_size_mlsgd):\n",
        "            XX = X[i*batch_size_mlsgd:(i+1)*batch_size_mlsgd]\n",
        "            yy = y[i*batch_size_mlsgd:(i+1)*batch_size_mlsgd]\n",
        "\n",
        "            # deepcopy and apply Gaussian perturbation\n",
        "            model_rand = deepcopy(model_fix)\n",
        "            model_rand.apply(GP)\n",
        "\n",
        "            # evaluate loss and gradient\n",
        "            pred = model_rand(XX)\n",
        "            loss = loss_fn(pred, yy)\n",
        "            loss.backward()\n",
        "\n",
        "            count = 0\n",
        "            for p in model.parameters():\n",
        "                p.data -= lr * list(model_rand.parameters())[count].grad.data\n",
        "                count += 1\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop_cuda(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.cuda()\n",
        "            y = y.cuda()\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "    return test_loss, correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcu_wOurL0Z5",
        "outputId": "b73de1ea-ddde-49c7-97ca-2ef464ab78e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch size:  160\n"
          ]
        }
      ],
      "source": [
        "print('batch size: ', batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M6ckH5qL4HR"
      },
      "source": [
        "## Training NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCRUqUJhL4v5",
        "outputId": "14f22c6c-e3dd-41da-8ff8-b5760f1c98e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_size:  16\n",
            "Nr:  10\n",
            "nu =  800\n",
            "rho =  1\n",
            "Test Error: \n",
            " Accuracy: 10.2%, Avg loss: 2.307298 \n",
            "\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.308937  [    0/50000]\n",
            "loss: 2.244109  [16000/50000]\n",
            "loss: 2.087219  [32000/50000]\n",
            "loss: 1.718348  [48000/50000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.884714  [    0/50000]\n",
            "loss: 1.750964  [16000/50000]\n",
            "loss: 1.944033  [32000/50000]\n",
            "loss: 1.853067  [48000/50000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.978785  [    0/50000]\n",
            "loss: 1.619598  [16000/50000]\n",
            "loss: 1.573739  [32000/50000]\n",
            "loss: 1.847487  [48000/50000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.635209  [    0/50000]\n",
            "loss: 2.060882  [16000/50000]\n",
            "loss: 1.461414  [32000/50000]\n",
            "loss: 1.752042  [48000/50000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.242077  [    0/50000]\n",
            "loss: 1.265465  [16000/50000]\n",
            "loss: 1.522411  [32000/50000]\n",
            "loss: 1.420921  [48000/50000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.477465  [    0/50000]\n",
            "loss: 1.650939  [16000/50000]\n",
            "loss: 1.077946  [32000/50000]\n",
            "loss: 1.841542  [48000/50000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.841600  [    0/50000]\n",
            "loss: 0.870589  [16000/50000]\n",
            "loss: 0.873049  [32000/50000]\n",
            "loss: 1.390383  [48000/50000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.534497  [    0/50000]\n",
            "loss: 1.602203  [16000/50000]\n",
            "loss: 0.894084  [32000/50000]\n",
            "loss: 1.429955  [48000/50000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.287204  [    0/50000]\n",
            "loss: 1.017632  [16000/50000]\n",
            "loss: 1.619973  [32000/50000]\n",
            "loss: 1.603081  [48000/50000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.222674  [    0/50000]\n",
            "loss: 1.228089  [16000/50000]\n",
            "loss: 1.751529  [32000/50000]\n",
            "loss: 1.108080  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 56.2%, Avg loss: 1.228833 \n",
            "\n",
            "nu =  800\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.606695  [    0/50000]\n",
            "loss: 0.712937  [16000/50000]\n",
            "loss: 0.786716  [32000/50000]\n",
            "loss: 1.508894  [48000/50000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.238737  [    0/50000]\n",
            "loss: 1.031268  [16000/50000]\n",
            "loss: 1.480632  [32000/50000]\n",
            "loss: 0.940229  [48000/50000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.755342  [    0/50000]\n",
            "loss: 1.128984  [16000/50000]\n",
            "loss: 0.995184  [32000/50000]\n",
            "loss: 1.308533  [48000/50000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.895470  [    0/50000]\n",
            "loss: 0.867749  [16000/50000]\n",
            "loss: 0.825527  [32000/50000]\n",
            "loss: 1.450494  [48000/50000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.174416  [    0/50000]\n",
            "loss: 1.057145  [16000/50000]\n",
            "loss: 1.121529  [32000/50000]\n",
            "loss: 1.813035  [48000/50000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.069149  [    0/50000]\n",
            "loss: 0.981930  [16000/50000]\n",
            "loss: 0.781166  [32000/50000]\n",
            "loss: 1.306445  [48000/50000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.924311  [    0/50000]\n",
            "loss: 1.228060  [16000/50000]\n",
            "loss: 0.918889  [32000/50000]\n",
            "loss: 0.940142  [48000/50000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.964148  [    0/50000]\n",
            "loss: 1.276787  [16000/50000]\n",
            "loss: 0.965058  [32000/50000]\n",
            "loss: 1.230397  [48000/50000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 1.298104  [    0/50000]\n",
            "loss: 1.419674  [16000/50000]\n",
            "loss: 1.128545  [32000/50000]\n",
            "loss: 1.100651  [48000/50000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1.475346  [    0/50000]\n",
            "loss: 0.916837  [16000/50000]\n",
            "loss: 1.330731  [32000/50000]\n",
            "loss: 1.023922  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 64.5%, Avg loss: 1.006505 \n",
            "\n",
            "nu =  800\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.713391  [    0/50000]\n",
            "loss: 1.536974  [16000/50000]\n",
            "loss: 1.115519  [32000/50000]\n",
            "loss: 0.585846  [48000/50000]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.813811  [    0/50000]\n",
            "loss: 1.374765  [16000/50000]\n",
            "loss: 0.801418  [32000/50000]\n",
            "loss: 0.977621  [48000/50000]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.795719  [    0/50000]\n",
            "loss: 1.344818  [16000/50000]\n",
            "loss: 1.016322  [32000/50000]\n",
            "loss: 1.264400  [48000/50000]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.906402  [    0/50000]\n",
            "loss: 0.892538  [16000/50000]\n",
            "loss: 1.075492  [32000/50000]\n",
            "loss: 0.636373  [48000/50000]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 1.238361  [    0/50000]\n",
            "loss: 0.747070  [16000/50000]\n",
            "loss: 1.007478  [32000/50000]\n",
            "loss: 1.250513  [48000/50000]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.693284  [    0/50000]\n",
            "loss: 1.312136  [16000/50000]\n",
            "loss: 0.839161  [32000/50000]\n",
            "loss: 1.087295  [48000/50000]\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.907819  [    0/50000]\n",
            "loss: 0.923238  [16000/50000]\n",
            "loss: 1.206085  [32000/50000]\n",
            "loss: 0.629429  [48000/50000]\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 1.312399  [    0/50000]\n",
            "loss: 0.913632  [16000/50000]\n",
            "loss: 0.909874  [32000/50000]\n",
            "loss: 0.824630  [48000/50000]\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.655857  [    0/50000]\n",
            "loss: 1.234725  [16000/50000]\n",
            "loss: 1.449637  [32000/50000]\n",
            "loss: 0.768040  [48000/50000]\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.428322  [    0/50000]\n",
            "loss: 0.755963  [16000/50000]\n",
            "loss: 1.079576  [32000/50000]\n",
            "loss: 0.621405  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 69.4%, Avg loss: 0.873951 \n",
            "\n",
            "nu =  800\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.716202  [    0/50000]\n",
            "loss: 1.567410  [16000/50000]\n",
            "loss: 0.973630  [32000/50000]\n",
            "loss: 1.086762  [48000/50000]\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.632876  [    0/50000]\n",
            "loss: 0.781727  [16000/50000]\n",
            "loss: 0.878801  [32000/50000]\n",
            "loss: 1.292152  [48000/50000]\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 1.250944  [    0/50000]\n",
            "loss: 1.174115  [16000/50000]\n",
            "loss: 0.609355  [32000/50000]\n",
            "loss: 0.616014  [48000/50000]\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.470190  [    0/50000]\n",
            "loss: 0.688406  [16000/50000]\n",
            "loss: 0.893234  [32000/50000]\n",
            "loss: 1.485158  [48000/50000]\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.805421  [    0/50000]\n",
            "loss: 1.509953  [16000/50000]\n",
            "loss: 0.983272  [32000/50000]\n",
            "loss: 1.037402  [48000/50000]\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 1.093899  [    0/50000]\n",
            "loss: 0.801174  [16000/50000]\n",
            "loss: 1.107430  [32000/50000]\n",
            "loss: 1.021275  [48000/50000]\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 1.022326  [    0/50000]\n",
            "loss: 0.642076  [16000/50000]\n",
            "loss: 0.862852  [32000/50000]\n",
            "loss: 0.857915  [48000/50000]\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.911146  [    0/50000]\n",
            "loss: 1.126180  [16000/50000]\n",
            "loss: 1.016279  [32000/50000]\n",
            "loss: 0.755819  [48000/50000]\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.796839  [    0/50000]\n",
            "loss: 1.120579  [16000/50000]\n",
            "loss: 1.074763  [32000/50000]\n",
            "loss: 0.707982  [48000/50000]\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.756084  [    0/50000]\n",
            "loss: 0.541316  [16000/50000]\n",
            "loss: 1.338212  [32000/50000]\n",
            "loss: 0.654026  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 74.1%, Avg loss: 0.740923 \n",
            "\n",
            "nu =  800\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.707375  [    0/50000]\n",
            "loss: 1.028164  [16000/50000]\n",
            "loss: 0.457765  [32000/50000]\n",
            "loss: 0.536599  [48000/50000]\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.702804  [    0/50000]\n",
            "loss: 1.049646  [16000/50000]\n",
            "loss: 0.765422  [32000/50000]\n",
            "loss: 0.971444  [48000/50000]\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 1.545065  [    0/50000]\n",
            "loss: 0.808128  [16000/50000]\n",
            "loss: 0.620777  [32000/50000]\n",
            "loss: 0.818238  [48000/50000]\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.911976  [    0/50000]\n",
            "loss: 0.631914  [16000/50000]\n",
            "loss: 0.466334  [32000/50000]\n",
            "loss: 0.873873  [48000/50000]\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.611360  [    0/50000]\n",
            "loss: 0.548573  [16000/50000]\n",
            "loss: 0.913247  [32000/50000]\n",
            "loss: 0.783056  [48000/50000]\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.868655  [    0/50000]\n",
            "loss: 0.529538  [16000/50000]\n",
            "loss: 0.457472  [32000/50000]\n",
            "loss: 0.927689  [48000/50000]\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 1.043855  [    0/50000]\n",
            "loss: 0.448770  [16000/50000]\n",
            "loss: 0.578253  [32000/50000]\n",
            "loss: 0.564712  [48000/50000]\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.455584  [    0/50000]\n",
            "loss: 1.086025  [16000/50000]\n",
            "loss: 0.442381  [32000/50000]\n",
            "loss: 1.040964  [48000/50000]\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.372805  [    0/50000]\n",
            "loss: 0.812676  [16000/50000]\n",
            "loss: 0.665774  [32000/50000]\n",
            "loss: 0.520454  [48000/50000]\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 1.022106  [    0/50000]\n",
            "loss: 0.805993  [16000/50000]\n",
            "loss: 0.632581  [32000/50000]\n",
            "loss: 0.629725  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 78.3%, Avg loss: 0.627306 \n",
            "\n",
            "nu =  800\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.768163  [    0/50000]\n",
            "loss: 0.489093  [16000/50000]\n",
            "loss: 0.482449  [32000/50000]\n",
            "loss: 1.238169  [48000/50000]\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.474530  [    0/50000]\n",
            "loss: 0.445049  [16000/50000]\n",
            "loss: 1.002174  [32000/50000]\n",
            "loss: 0.820703  [48000/50000]\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.208474  [    0/50000]\n",
            "loss: 0.867904  [16000/50000]\n",
            "loss: 0.473350  [32000/50000]\n",
            "loss: 0.732372  [48000/50000]\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.131040  [    0/50000]\n",
            "loss: 0.817069  [16000/50000]\n",
            "loss: 0.287417  [32000/50000]\n",
            "loss: 1.258944  [48000/50000]\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.659447  [    0/50000]\n",
            "loss: 0.542080  [16000/50000]\n",
            "loss: 1.071465  [32000/50000]\n",
            "loss: 0.629071  [48000/50000]\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.429193  [    0/50000]\n",
            "loss: 0.899037  [16000/50000]\n",
            "loss: 0.831787  [32000/50000]\n",
            "loss: 0.763416  [48000/50000]\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.556036  [    0/50000]\n",
            "loss: 1.040681  [16000/50000]\n",
            "loss: 0.512762  [32000/50000]\n",
            "loss: 0.541421  [48000/50000]\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.409862  [    0/50000]\n",
            "loss: 0.752267  [16000/50000]\n",
            "loss: 0.660621  [32000/50000]\n",
            "loss: 0.489116  [48000/50000]\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.387235  [    0/50000]\n",
            "loss: 0.809356  [16000/50000]\n",
            "loss: 0.441125  [32000/50000]\n",
            "loss: 0.505828  [48000/50000]\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.592206  [    0/50000]\n",
            "loss: 1.071689  [16000/50000]\n",
            "loss: 0.782027  [32000/50000]\n",
            "loss: 0.580851  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 81.0%, Avg loss: 0.550900 \n",
            "\n",
            "nu =  800\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.356651  [    0/50000]\n",
            "loss: 0.685382  [16000/50000]\n",
            "loss: 0.722095  [32000/50000]\n",
            "loss: 0.976224  [48000/50000]\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.452207  [    0/50000]\n",
            "loss: 0.503229  [16000/50000]\n",
            "loss: 0.545647  [32000/50000]\n",
            "loss: 0.573281  [48000/50000]\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.558471  [    0/50000]\n",
            "loss: 0.516029  [16000/50000]\n",
            "loss: 0.269931  [32000/50000]\n",
            "loss: 0.478645  [48000/50000]\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.421566  [    0/50000]\n",
            "loss: 0.735883  [16000/50000]\n",
            "loss: 0.455680  [32000/50000]\n",
            "loss: 0.645182  [48000/50000]\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.625726  [    0/50000]\n",
            "loss: 0.586979  [16000/50000]\n",
            "loss: 0.957163  [32000/50000]\n",
            "loss: 0.581045  [48000/50000]\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.363943  [    0/50000]\n",
            "loss: 0.400354  [16000/50000]\n",
            "loss: 0.505005  [32000/50000]\n",
            "loss: 0.758587  [48000/50000]\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.682972  [    0/50000]\n",
            "loss: 0.322320  [16000/50000]\n",
            "loss: 0.454100  [32000/50000]\n",
            "loss: 1.257317  [48000/50000]\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.640489  [    0/50000]\n",
            "loss: 0.229453  [16000/50000]\n",
            "loss: 0.950133  [32000/50000]\n",
            "loss: 0.677687  [48000/50000]\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.864268  [    0/50000]\n",
            "loss: 0.834145  [16000/50000]\n",
            "loss: 0.360132  [32000/50000]\n",
            "loss: 0.283994  [48000/50000]\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.568034  [    0/50000]\n",
            "loss: 0.787777  [16000/50000]\n",
            "loss: 0.242681  [32000/50000]\n",
            "loss: 0.817970  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.451279 \n",
            "\n",
            "nu =  800\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.396234  [    0/50000]\n",
            "loss: 0.728902  [16000/50000]\n",
            "loss: 0.566266  [32000/50000]\n",
            "loss: 0.765984  [48000/50000]\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.431471  [    0/50000]\n",
            "loss: 0.574725  [16000/50000]\n",
            "loss: 0.650997  [32000/50000]\n",
            "loss: 0.209420  [48000/50000]\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.812039  [    0/50000]\n",
            "loss: 0.336192  [16000/50000]\n",
            "loss: 0.424724  [32000/50000]\n",
            "loss: 0.629064  [48000/50000]\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.582835  [    0/50000]\n",
            "loss: 0.575403  [16000/50000]\n",
            "loss: 0.772905  [32000/50000]\n",
            "loss: 0.284478  [48000/50000]\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.582124  [    0/50000]\n",
            "loss: 0.494043  [16000/50000]\n",
            "loss: 0.495782  [32000/50000]\n",
            "loss: 0.420102  [48000/50000]\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.863112  [    0/50000]\n",
            "loss: 0.368691  [16000/50000]\n",
            "loss: 0.932029  [32000/50000]\n",
            "loss: 0.439572  [48000/50000]\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.450135  [    0/50000]\n",
            "loss: 0.639216  [16000/50000]\n",
            "loss: 0.240232  [32000/50000]\n",
            "loss: 0.363942  [48000/50000]\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.206343  [    0/50000]\n",
            "loss: 0.154501  [16000/50000]\n",
            "loss: 0.326165  [32000/50000]\n",
            "loss: 0.776065  [48000/50000]\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.353656  [    0/50000]\n",
            "loss: 0.121833  [16000/50000]\n",
            "loss: 0.587600  [32000/50000]\n",
            "loss: 0.279593  [48000/50000]\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.355946  [    0/50000]\n",
            "loss: 0.499731  [16000/50000]\n",
            "loss: 0.898048  [32000/50000]\n",
            "loss: 0.731010  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 86.1%, Avg loss: 0.405821 \n",
            "\n",
            "nu =  800\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.377334  [    0/50000]\n",
            "loss: 0.365852  [16000/50000]\n",
            "loss: 0.383461  [32000/50000]\n",
            "loss: 0.386559  [48000/50000]\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.542019  [    0/50000]\n",
            "loss: 0.180297  [16000/50000]\n",
            "loss: 0.208192  [32000/50000]\n",
            "loss: 0.260935  [48000/50000]\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.291416  [    0/50000]\n",
            "loss: 0.454341  [16000/50000]\n",
            "loss: 0.628776  [32000/50000]\n",
            "loss: 0.367981  [48000/50000]\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.225076  [    0/50000]\n",
            "loss: 0.584806  [16000/50000]\n",
            "loss: 0.345210  [32000/50000]\n",
            "loss: 0.766058  [48000/50000]\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.415015  [    0/50000]\n",
            "loss: 0.148878  [16000/50000]\n",
            "loss: 0.429964  [32000/50000]\n",
            "loss: 0.506195  [48000/50000]\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.319314  [    0/50000]\n",
            "loss: 0.382670  [16000/50000]\n",
            "loss: 0.423813  [32000/50000]\n",
            "loss: 0.368610  [48000/50000]\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.164884  [    0/50000]\n",
            "loss: 0.557730  [16000/50000]\n",
            "loss: 0.597914  [32000/50000]\n",
            "loss: 0.418531  [48000/50000]\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.459839  [    0/50000]\n",
            "loss: 0.496915  [16000/50000]\n",
            "loss: 0.226826  [32000/50000]\n",
            "loss: 0.899312  [48000/50000]\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.372438  [    0/50000]\n",
            "loss: 0.171386  [16000/50000]\n",
            "loss: 0.657975  [32000/50000]\n",
            "loss: 0.416011  [48000/50000]\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.458184  [    0/50000]\n",
            "loss: 0.449548  [16000/50000]\n",
            "loss: 0.617485  [32000/50000]\n",
            "loss: 0.175979  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 88.6%, Avg loss: 0.343243 \n",
            "\n",
            "nu =  800\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.280466  [    0/50000]\n",
            "loss: 0.520576  [16000/50000]\n",
            "loss: 0.494886  [32000/50000]\n",
            "loss: 0.394354  [48000/50000]\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.378677  [    0/50000]\n",
            "loss: 0.415208  [16000/50000]\n",
            "loss: 0.249781  [32000/50000]\n",
            "loss: 0.462312  [48000/50000]\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.406076  [    0/50000]\n",
            "loss: 0.118393  [16000/50000]\n",
            "loss: 0.754497  [32000/50000]\n",
            "loss: 0.323595  [48000/50000]\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 0.367984  [    0/50000]\n",
            "loss: 0.375634  [16000/50000]\n",
            "loss: 0.145236  [32000/50000]\n",
            "loss: 0.377659  [48000/50000]\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 0.325668  [    0/50000]\n",
            "loss: 0.303890  [16000/50000]\n",
            "loss: 0.381309  [32000/50000]\n",
            "loss: 0.705119  [48000/50000]\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 0.237946  [    0/50000]\n",
            "loss: 0.345832  [16000/50000]\n",
            "loss: 0.106662  [32000/50000]\n",
            "loss: 0.590448  [48000/50000]\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 0.277130  [    0/50000]\n",
            "loss: 0.186865  [16000/50000]\n",
            "loss: 0.417586  [32000/50000]\n",
            "loss: 0.094842  [48000/50000]\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 0.769086  [    0/50000]\n",
            "loss: 0.070414  [16000/50000]\n",
            "loss: 0.669719  [32000/50000]\n",
            "loss: 0.437643  [48000/50000]\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 0.339025  [    0/50000]\n",
            "loss: 0.837119  [16000/50000]\n",
            "loss: 0.771619  [32000/50000]\n",
            "loss: 0.411663  [48000/50000]\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 0.352247  [    0/50000]\n",
            "loss: 0.366844  [16000/50000]\n",
            "loss: 0.427622  [32000/50000]\n",
            "loss: 0.440808  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 90.4%, Avg loss: 0.293406 \n",
            "\n",
            "nu =  800\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "loss: 0.339188  [    0/50000]\n",
            "loss: 0.595869  [16000/50000]\n",
            "loss: 0.185219  [32000/50000]\n",
            "loss: 0.352621  [48000/50000]\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "loss: 0.357722  [    0/50000]\n",
            "loss: 0.351309  [16000/50000]\n",
            "loss: 0.476307  [32000/50000]\n",
            "loss: 0.448115  [48000/50000]\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "loss: 0.354519  [    0/50000]\n",
            "loss: 0.123261  [16000/50000]\n",
            "loss: 0.321526  [32000/50000]\n",
            "loss: 0.370894  [48000/50000]\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "loss: 0.062767  [    0/50000]\n",
            "loss: 0.293178  [16000/50000]\n",
            "loss: 0.281056  [32000/50000]\n",
            "loss: 0.337815  [48000/50000]\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "loss: 0.292329  [    0/50000]\n",
            "loss: 0.220864  [16000/50000]\n",
            "loss: 0.430562  [32000/50000]\n",
            "loss: 0.734159  [48000/50000]\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "loss: 0.168277  [    0/50000]\n",
            "loss: 0.447174  [16000/50000]\n",
            "loss: 0.277426  [32000/50000]\n",
            "loss: 0.351156  [48000/50000]\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "loss: 0.151184  [    0/50000]\n",
            "loss: 0.321501  [16000/50000]\n",
            "loss: 0.289946  [32000/50000]\n",
            "loss: 0.400411  [48000/50000]\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "loss: 0.232602  [    0/50000]\n",
            "loss: 0.104990  [16000/50000]\n",
            "loss: 0.340681  [32000/50000]\n",
            "loss: 0.398213  [48000/50000]\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "loss: 0.134152  [    0/50000]\n",
            "loss: 0.272672  [16000/50000]\n",
            "loss: 0.506469  [32000/50000]\n",
            "loss: 0.289629  [48000/50000]\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "loss: 0.101557  [    0/50000]\n",
            "loss: 0.445766  [16000/50000]\n",
            "loss: 0.200581  [32000/50000]\n",
            "loss: 0.489516  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 90.7%, Avg loss: 0.276872 \n",
            "\n",
            "nu =  800\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "loss: 0.410684  [    0/50000]\n",
            "loss: 0.142143  [16000/50000]\n",
            "loss: 0.560279  [32000/50000]\n",
            "loss: 0.137274  [48000/50000]\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "loss: 0.054802  [    0/50000]\n",
            "loss: 0.545597  [16000/50000]\n",
            "loss: 0.223245  [32000/50000]\n",
            "loss: 0.469007  [48000/50000]\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "loss: 0.057009  [    0/50000]\n",
            "loss: 0.171051  [16000/50000]\n",
            "loss: 0.212506  [32000/50000]\n",
            "loss: 0.317441  [48000/50000]\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "loss: 0.090208  [    0/50000]\n",
            "loss: 0.401633  [16000/50000]\n",
            "loss: 0.405929  [32000/50000]\n",
            "loss: 0.188493  [48000/50000]\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "loss: 0.143103  [    0/50000]\n",
            "loss: 0.803847  [16000/50000]\n",
            "loss: 0.540356  [32000/50000]\n",
            "loss: 0.201595  [48000/50000]\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "loss: 0.197892  [    0/50000]\n",
            "loss: 0.179246  [16000/50000]\n",
            "loss: 0.125016  [32000/50000]\n",
            "loss: 0.216672  [48000/50000]\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "loss: 0.263048  [    0/50000]\n",
            "loss: 0.050662  [16000/50000]\n",
            "loss: 0.308674  [32000/50000]\n",
            "loss: 0.284397  [48000/50000]\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "loss: 0.262235  [    0/50000]\n",
            "loss: 0.157108  [16000/50000]\n",
            "loss: 0.314456  [32000/50000]\n",
            "loss: 0.210851  [48000/50000]\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "loss: 0.348013  [    0/50000]\n",
            "loss: 0.551461  [16000/50000]\n",
            "loss: 0.248908  [32000/50000]\n",
            "loss: 0.366102  [48000/50000]\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "loss: 0.281554  [    0/50000]\n",
            "loss: 0.347628  [16000/50000]\n",
            "loss: 0.361702  [32000/50000]\n",
            "loss: 0.287881  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 92.0%, Avg loss: 0.241854 \n",
            "\n",
            "nu =  800\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "loss: 0.362966  [    0/50000]\n",
            "loss: 0.200251  [16000/50000]\n",
            "loss: 0.106886  [32000/50000]\n",
            "loss: 0.164651  [48000/50000]\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "loss: 0.082296  [    0/50000]\n",
            "loss: 0.112086  [16000/50000]\n",
            "loss: 0.159560  [32000/50000]\n",
            "loss: 0.236299  [48000/50000]\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "loss: 0.139968  [    0/50000]\n",
            "loss: 0.187652  [16000/50000]\n",
            "loss: 0.181617  [32000/50000]\n",
            "loss: 0.377466  [48000/50000]\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "loss: 0.162325  [    0/50000]\n",
            "loss: 0.496452  [16000/50000]\n",
            "loss: 0.215309  [32000/50000]\n",
            "loss: 0.339587  [48000/50000]\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "loss: 0.216525  [    0/50000]\n",
            "loss: 0.358053  [16000/50000]\n",
            "loss: 0.294767  [32000/50000]\n",
            "loss: 0.502106  [48000/50000]\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "loss: 0.145158  [    0/50000]\n",
            "loss: 0.080458  [16000/50000]\n",
            "loss: 0.298668  [32000/50000]\n",
            "loss: 0.144150  [48000/50000]\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "loss: 0.075454  [    0/50000]\n",
            "loss: 0.729551  [16000/50000]\n",
            "loss: 0.317674  [32000/50000]\n",
            "loss: 0.402428  [48000/50000]\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "loss: 0.185387  [    0/50000]\n",
            "loss: 0.353352  [16000/50000]\n",
            "loss: 0.227484  [32000/50000]\n",
            "loss: 0.075109  [48000/50000]\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "loss: 0.559750  [    0/50000]\n",
            "loss: 0.921746  [16000/50000]\n",
            "loss: 0.148646  [32000/50000]\n",
            "loss: 0.267999  [48000/50000]\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "loss: 0.600003  [    0/50000]\n",
            "loss: 0.344588  [16000/50000]\n",
            "loss: 0.387303  [32000/50000]\n",
            "loss: 0.125193  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 93.3%, Avg loss: 0.211698 \n",
            "\n",
            "nu =  800\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "loss: 0.205691  [    0/50000]\n",
            "loss: 0.167148  [16000/50000]\n",
            "loss: 0.246651  [32000/50000]\n",
            "loss: 0.256718  [48000/50000]\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "loss: 0.092554  [    0/50000]\n",
            "loss: 0.152825  [16000/50000]\n",
            "loss: 0.276113  [32000/50000]\n",
            "loss: 0.378243  [48000/50000]\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "loss: 0.375940  [    0/50000]\n",
            "loss: 0.635202  [16000/50000]\n",
            "loss: 0.366729  [32000/50000]\n",
            "loss: 0.661748  [48000/50000]\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "loss: 0.148116  [    0/50000]\n",
            "loss: 0.225711  [16000/50000]\n",
            "loss: 0.528100  [32000/50000]\n",
            "loss: 0.935569  [48000/50000]\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "loss: 0.262806  [    0/50000]\n",
            "loss: 0.114456  [16000/50000]\n",
            "loss: 0.207874  [32000/50000]\n",
            "loss: 0.620588  [48000/50000]\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "loss: 0.170152  [    0/50000]\n",
            "loss: 0.250294  [16000/50000]\n",
            "loss: 0.533661  [32000/50000]\n",
            "loss: 0.211200  [48000/50000]\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "loss: 0.160680  [    0/50000]\n",
            "loss: 0.219801  [16000/50000]\n",
            "loss: 0.221783  [32000/50000]\n",
            "loss: 0.645390  [48000/50000]\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "loss: 0.227607  [    0/50000]\n",
            "loss: 0.554582  [16000/50000]\n",
            "loss: 0.142111  [32000/50000]\n",
            "loss: 0.111251  [48000/50000]\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "loss: 0.149628  [    0/50000]\n",
            "loss: 0.146835  [16000/50000]\n",
            "loss: 1.129257  [32000/50000]\n",
            "loss: 0.939581  [48000/50000]\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "loss: 0.130020  [    0/50000]\n",
            "loss: 0.150840  [16000/50000]\n",
            "loss: 0.044960  [32000/50000]\n",
            "loss: 0.085129  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 94.0%, Avg loss: 0.188325 \n",
            "\n",
            "nu =  800\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "loss: 0.376368  [    0/50000]\n",
            "loss: 0.214870  [16000/50000]\n",
            "loss: 0.646811  [32000/50000]\n",
            "loss: 0.480082  [48000/50000]\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "loss: 0.140625  [    0/50000]\n",
            "loss: 0.230711  [16000/50000]\n",
            "loss: 0.230697  [32000/50000]\n",
            "loss: 0.294726  [48000/50000]\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "loss: 0.271342  [    0/50000]\n",
            "loss: 0.260681  [16000/50000]\n",
            "loss: 0.203378  [32000/50000]\n",
            "loss: 2.150712  [48000/50000]\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "loss: 0.261456  [    0/50000]\n",
            "loss: 0.131564  [16000/50000]\n",
            "loss: 0.064196  [32000/50000]\n",
            "loss: 0.248293  [48000/50000]\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "loss: 0.164290  [    0/50000]\n",
            "loss: 0.232744  [16000/50000]\n",
            "loss: 0.583736  [32000/50000]\n",
            "loss: 0.515405  [48000/50000]\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "loss: 0.121377  [    0/50000]\n",
            "loss: 0.170342  [16000/50000]\n",
            "loss: 0.272295  [32000/50000]\n",
            "loss: 0.625539  [48000/50000]\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "loss: 0.107289  [    0/50000]\n",
            "loss: 0.242835  [16000/50000]\n",
            "loss: 0.116768  [32000/50000]\n",
            "loss: 0.755724  [48000/50000]\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "loss: 0.223444  [    0/50000]\n",
            "loss: 0.108757  [16000/50000]\n",
            "loss: 0.136721  [32000/50000]\n",
            "loss: 0.256839  [48000/50000]\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "loss: 0.168636  [    0/50000]\n",
            "loss: 0.141403  [16000/50000]\n",
            "loss: 0.804819  [32000/50000]\n",
            "loss: 0.223013  [48000/50000]\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "loss: 0.070237  [    0/50000]\n",
            "loss: 0.120259  [16000/50000]\n",
            "loss: 0.273525  [32000/50000]\n",
            "loss: 0.332643  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 92.2%, Avg loss: 0.220509 \n",
            "\n",
            "nu =  800\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "loss: 0.513156  [    0/50000]\n",
            "loss: 0.178821  [16000/50000]\n",
            "loss: 0.124690  [32000/50000]\n",
            "loss: 0.153799  [48000/50000]\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "loss: 0.227212  [    0/50000]\n",
            "loss: 0.143355  [16000/50000]\n",
            "loss: 0.044685  [32000/50000]\n",
            "loss: 0.125853  [48000/50000]\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "loss: 0.835018  [    0/50000]\n",
            "loss: 0.068043  [16000/50000]\n",
            "loss: 0.168110  [32000/50000]\n",
            "loss: 0.047506  [48000/50000]\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "loss: 0.217367  [    0/50000]\n",
            "loss: 0.206438  [16000/50000]\n",
            "loss: 0.284840  [32000/50000]\n",
            "loss: 0.357140  [48000/50000]\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "loss: 0.365436  [    0/50000]\n",
            "loss: 0.135788  [16000/50000]\n",
            "loss: 0.199048  [32000/50000]\n",
            "loss: 0.163814  [48000/50000]\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "loss: 0.071172  [    0/50000]\n",
            "loss: 0.246724  [16000/50000]\n",
            "loss: 0.209261  [32000/50000]\n",
            "loss: 0.062908  [48000/50000]\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "loss: 0.225034  [    0/50000]\n",
            "loss: 0.460802  [16000/50000]\n",
            "loss: 0.381809  [32000/50000]\n",
            "loss: 0.458404  [48000/50000]\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "loss: 0.120920  [    0/50000]\n",
            "loss: 1.084601  [16000/50000]\n",
            "loss: 0.383921  [32000/50000]\n",
            "loss: 0.314499  [48000/50000]\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "loss: 0.100568  [    0/50000]\n",
            "loss: 0.371770  [16000/50000]\n",
            "loss: 0.059608  [32000/50000]\n",
            "loss: 0.028520  [48000/50000]\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "loss: 0.094310  [    0/50000]\n",
            "loss: 0.101397  [16000/50000]\n",
            "loss: 0.066480  [32000/50000]\n",
            "loss: 0.473804  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 0.109803 \n",
            "\n",
            "nu =  800\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "loss: 0.131250  [    0/50000]\n",
            "loss: 0.158373  [16000/50000]\n",
            "loss: 0.198078  [32000/50000]\n",
            "loss: 0.631129  [48000/50000]\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "loss: 0.215912  [    0/50000]\n",
            "loss: 0.318754  [16000/50000]\n",
            "loss: 0.707533  [32000/50000]\n",
            "loss: 0.428315  [48000/50000]\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "loss: 0.089710  [    0/50000]\n",
            "loss: 0.162808  [16000/50000]\n",
            "loss: 0.165588  [32000/50000]\n",
            "loss: 0.246082  [48000/50000]\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "loss: 0.196106  [    0/50000]\n",
            "loss: 0.475288  [16000/50000]\n",
            "loss: 0.563915  [32000/50000]\n",
            "loss: 0.140433  [48000/50000]\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "loss: 0.413973  [    0/50000]\n",
            "loss: 0.255437  [16000/50000]\n",
            "loss: 0.447767  [32000/50000]\n",
            "loss: 0.334887  [48000/50000]\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "loss: 0.052232  [    0/50000]\n",
            "loss: 0.109744  [16000/50000]\n",
            "loss: 0.069817  [32000/50000]\n",
            "loss: 0.619120  [48000/50000]\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "loss: 0.558913  [    0/50000]\n",
            "loss: 0.466042  [16000/50000]\n",
            "loss: 0.096952  [32000/50000]\n",
            "loss: 0.072245  [48000/50000]\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "loss: 0.555176  [    0/50000]\n",
            "loss: 0.171784  [16000/50000]\n",
            "loss: 0.220820  [32000/50000]\n",
            "loss: 0.063132  [48000/50000]\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "loss: 0.088026  [    0/50000]\n",
            "loss: 0.097575  [16000/50000]\n",
            "loss: 0.180273  [32000/50000]\n",
            "loss: 0.327474  [48000/50000]\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "loss: 0.043772  [    0/50000]\n",
            "loss: 0.041068  [16000/50000]\n",
            "loss: 0.101134  [32000/50000]\n",
            "loss: 0.388347  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 85.9%, Avg loss: 0.413076 \n",
            "\n",
            "nu =  800\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "loss: 0.400247  [    0/50000]\n",
            "loss: 0.057964  [16000/50000]\n",
            "loss: 1.231186  [32000/50000]\n",
            "loss: 0.084046  [48000/50000]\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "loss: 0.165690  [    0/50000]\n",
            "loss: 0.109955  [16000/50000]\n",
            "loss: 0.146814  [32000/50000]\n",
            "loss: 0.207778  [48000/50000]\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "loss: 0.500783  [    0/50000]\n",
            "loss: 0.486751  [16000/50000]\n",
            "loss: 0.198120  [32000/50000]\n",
            "loss: 0.676629  [48000/50000]\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "loss: 0.201372  [    0/50000]\n",
            "loss: 0.044310  [16000/50000]\n",
            "loss: 0.056077  [32000/50000]\n",
            "loss: 0.104562  [48000/50000]\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "loss: 0.060583  [    0/50000]\n",
            "loss: 0.066541  [16000/50000]\n",
            "loss: 0.170765  [32000/50000]\n",
            "loss: 0.041303  [48000/50000]\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "loss: 0.049994  [    0/50000]\n",
            "loss: 0.075570  [16000/50000]\n",
            "loss: 0.065947  [32000/50000]\n",
            "loss: 0.031112  [48000/50000]\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "loss: 0.013567  [    0/50000]\n",
            "loss: 0.056839  [16000/50000]\n",
            "loss: 0.046015  [32000/50000]\n",
            "loss: 0.076744  [48000/50000]\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "loss: 0.023840  [    0/50000]\n",
            "loss: 0.037002  [16000/50000]\n",
            "loss: 0.095990  [32000/50000]\n",
            "loss: 0.045659  [48000/50000]\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "loss: 0.066616  [    0/50000]\n",
            "loss: 0.058462  [16000/50000]\n",
            "loss: 0.042505  [32000/50000]\n",
            "loss: 0.031899  [48000/50000]\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "loss: 0.018538  [    0/50000]\n",
            "loss: 0.045061  [16000/50000]\n",
            "loss: 0.027015  [32000/50000]\n",
            "loss: 0.562913  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 89.8%, Avg loss: 0.288451 \n",
            "\n",
            "nu =  800\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "loss: 0.300419  [    0/50000]\n",
            "loss: 0.726354  [16000/50000]\n",
            "loss: 0.500586  [32000/50000]\n",
            "loss: 0.424344  [48000/50000]\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "loss: 0.626401  [    0/50000]\n",
            "loss: 0.547613  [16000/50000]\n",
            "loss: 0.298552  [32000/50000]\n",
            "loss: 0.426801  [48000/50000]\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "loss: 0.073037  [    0/50000]\n",
            "loss: 0.120936  [16000/50000]\n",
            "loss: 0.123544  [32000/50000]\n",
            "loss: 0.048171  [48000/50000]\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "loss: 0.025759  [    0/50000]\n",
            "loss: 0.212947  [16000/50000]\n",
            "loss: 0.182558  [32000/50000]\n",
            "loss: 0.156929  [48000/50000]\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "loss: 0.093014  [    0/50000]\n",
            "loss: 0.084018  [16000/50000]\n",
            "loss: 0.185159  [32000/50000]\n",
            "loss: 0.277815  [48000/50000]\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "loss: 0.888777  [    0/50000]\n",
            "loss: 0.027683  [16000/50000]\n",
            "loss: 0.232601  [32000/50000]\n",
            "loss: 0.558185  [48000/50000]\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "loss: 0.020784  [    0/50000]\n",
            "loss: 0.065350  [16000/50000]\n",
            "loss: 0.113384  [32000/50000]\n",
            "loss: 0.073368  [48000/50000]\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "loss: 0.050296  [    0/50000]\n",
            "loss: 0.047221  [16000/50000]\n",
            "loss: 0.070468  [32000/50000]\n",
            "loss: 0.125306  [48000/50000]\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "loss: 0.033847  [    0/50000]\n",
            "loss: 0.042070  [16000/50000]\n",
            "loss: 0.039798  [32000/50000]\n",
            "loss: 0.016100  [48000/50000]\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "loss: 0.023362  [    0/50000]\n",
            "loss: 0.019984  [16000/50000]\n",
            "loss: 0.020191  [32000/50000]\n",
            "loss: 0.061434  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.032826 \n",
            "\n",
            "nu =  800\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "loss: 0.051636  [    0/50000]\n",
            "loss: 0.023165  [16000/50000]\n",
            "loss: 0.043069  [32000/50000]\n",
            "loss: 0.059133  [48000/50000]\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "loss: 0.072988  [    0/50000]\n",
            "loss: 0.051117  [16000/50000]\n",
            "loss: 0.065366  [32000/50000]\n",
            "loss: 0.024232  [48000/50000]\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "loss: 0.024185  [    0/50000]\n",
            "loss: 0.029313  [16000/50000]\n",
            "loss: 0.023783  [32000/50000]\n",
            "loss: 0.033637  [48000/50000]\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "loss: 0.011036  [    0/50000]\n",
            "loss: 0.029714  [16000/50000]\n",
            "loss: 0.040541  [32000/50000]\n",
            "loss: 0.025193  [48000/50000]\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "loss: 0.028414  [    0/50000]\n",
            "loss: 0.032691  [16000/50000]\n",
            "loss: 0.125976  [32000/50000]\n",
            "loss: 0.021366  [48000/50000]\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "loss: 0.028439  [    0/50000]\n",
            "loss: 0.015439  [16000/50000]\n",
            "loss: 0.012865  [32000/50000]\n",
            "loss: 0.051880  [48000/50000]\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "loss: 0.032893  [    0/50000]\n",
            "loss: 0.005718  [16000/50000]\n",
            "loss: 0.023412  [32000/50000]\n",
            "loss: 0.043745  [48000/50000]\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "loss: 0.017858  [    0/50000]\n",
            "loss: 0.011782  [16000/50000]\n",
            "loss: 0.009483  [32000/50000]\n",
            "loss: 0.021491  [48000/50000]\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "loss: 0.029016  [    0/50000]\n",
            "loss: 0.014125  [16000/50000]\n",
            "loss: 0.049246  [32000/50000]\n",
            "loss: 0.021714  [48000/50000]\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "loss: 0.043633  [    0/50000]\n",
            "loss: 0.027725  [16000/50000]\n",
            "loss: 0.023973  [32000/50000]\n",
            "loss: 0.037491  [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 100.0%, Avg loss: 0.019140 \n",
            "\n",
            "nu =  800\n",
            "Done!\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "seed_torch()\n",
        "\n",
        "# model = torch.load('model_adam_20epoch.pth')\n",
        "model = LeNet5(10)\n",
        "model = model.cuda()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = loss_fn.cuda()\n",
        "\n",
        "learning_rate = 1e-2\n",
        "nu = 800\n",
        "epochs = 200\n",
        "Nr = 10\n",
        "rho = 1\n",
        "\n",
        "print('batch_size: ', batch_size // Nr)\n",
        "print('Nr: ', Nr)\n",
        "print('nu = ', nu)\n",
        "print('rho = ', rho)\n",
        "\n",
        "# record\n",
        "loss_vals = []\n",
        "loss_val, accuracy_val = test_loop_cuda(train_dataloader, model, loss_fn)\n",
        "loss_vals.append(loss_val)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    \n",
        "    train_loop_MLSGD(train_dataloader, model, loss_fn, learning_rate, Nr, batch_size, nu)\n",
        "\n",
        "    if np.remainder(t+1, 10) == 0:\n",
        "        loss_val, accuracy_val = test_loop_cuda(train_dataloader, model, loss_fn)\n",
        "        loss_vals.append(loss_val)\n",
        "        nu = rho * nu // 1\n",
        "        print(\"nu = \", nu)\n",
        "\n",
        "print(\"Done!\")\n",
        "\n",
        "epochs = np.linspace(0,epochs,len(loss_vals))\n",
        "plt.plot(epochs, loss_vals)\n",
        "np.save(\"lenet5_mlsgd_nu_800_rho_1_Nr_10_bs_16_lr_1e-2.npy\", loss_vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feurnZK0pZTj",
        "outputId": "d3474a91-d20c-4511-acc8-d4c8353cb869"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "337.0"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# batch = 32\n",
        "# 10epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xy8ZOHTzMb7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "ML-SGD_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}